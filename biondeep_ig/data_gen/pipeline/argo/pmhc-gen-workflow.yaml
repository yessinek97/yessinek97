apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: pmhc-gen-20220131-
  namespace: biondeep-ig-infra
  labels:
    workflows.argoproj.io/controller-instanceid: ichor

spec:
  entrypoint: main

  ########################
  # volume specification #
  ########################

  # volumeClaimTemplates:
  #   - metadata:
  #       name: biondeep-ig-pdb-data
  #     spec:
  #       accessModes: [ "ReadWriteMany"]
  #       storageClassName: rook-ceph-fs
  #       resources:
  #         requests:
  #           storage: 4048Gi

  # ###############################
  # # misc workflow configuration #
  # ###############################

  # Uncomment to delete pods on completion
  # podGC:
  #   strategy: OnWorkflowCompletion

  ###############################
  # global argument declaration #
  ###############################

  arguments:
    parameters:
      # core docker image configuration
      - name: core-image
        value: "" # Set by the GitLab CI once the Docker image is built
      - name: core-image-pull-strategy
        value: "IfNotPresent"

      # object storage connection
      - name: storage-endpoint
        value: "https://s3.kao.instadeep.io" #"http://192.168.20.1:30930"
      - name: storage-secret-name
        value: "biondeep-data"
      - name: input-data-bucket
        value: "biondeep-data"

      # object storage paths
      - name: pmhc-structures-s3-path
        value: "s3://biondeep-data/classi_data_for_instadeep/ig_tcr_pmhc/pmhc_structures.zip"

      - name: pmhc-data-s3-path
        value: "s3://biondeep-data/classi_data_for_instadeep/ig_tcr_pmhc/pmhc_data.csv"

      - name: upload-s3-path
        value: "s3://biondeep-data/classi_data_for_instadeep/ig_tcr_pmhc"

      # volume config
      - name: pvc-name
        value: "biondeep-ig-data"

      # workflow config
      - name: num-parallel-steps # Number of nodes
        value: "12"

      - name: num-runs-per-parallel-step # Number of entries per node
        value: "4875"

      - name: train-data-generator-cpus # Number of cpus per node
        value: "100"

  ###########################
  # workflow implementation #
  ###########################

  templates:
    #######################
    # main DAG definition #
    #######################

    - name: main
      dag:
        tasks:
          - name: pull-data
            template: pull-data
            arguments:
              parameters:
                - name: object-bucket
                  value: "{{workflow.parameters.input-data-bucket}}"
                - name: pvc-name
                  value: "{{workflow.parameters.pvc-name}}"
                - name: pmhc-structures-s3-path
                  value: "{{workflow.parameters.pmhc-structures-s3-path}}"
                - name: pmhc-data-s3-path
                  value: "{{workflow.parameters.pmhc-data-s3-path}}"

          - name: distribute-data-pmhc-gen
            template: distribute-data-pmhc-gen
            dependencies: [pull-data]
            arguments:
              parameters:
                - name: object-bucket
                  value: "{{workflow.parameters.input-data-bucket}}"
                - name: pvc-name
                  value: "{{workflow.parameters.pvc-name}}"

          - name: pmhc-gen
            template: pmhc-gen
            dependencies: [distribute-data-pmhc-gen]
            arguments:
              parameters:
                - name: object-bucket
                  value: "{{workflow.parameters.input-data-bucket}}"
                - name: pvc-name
                  value: "{{workflow.parameters.pvc-name}}"
                - name: todo_alleles_peptides_filename
                  value: "{{item}}"
            withParam: "{{tasks.distribute-data-pmhc-gen.outputs.result}}" # iterate over list

          - name: sync-results-to-s3
            template: sync-results-to-s3
            dependencies: [pmhc-gen]
            arguments:
              parameters:
                - name: object-bucket
                  value: "{{workflow.parameters.input-data-bucket}}"
                - name: pvc-name
                  value: "{{workflow.parameters.pvc-name}}"

    ##################
    # step templates #
    ##################
    - name: pull-data
      retryStrategy:
        limit: 5
        retryPolicy: "Always"
      inputs:
        parameters:
          - name: object-bucket
          - name: pvc-name
          - name: pmhc-structures-s3-path
          - name: pmhc-data-s3-path
      volumes:
        - name: biondeep-ig-pdb-data
          persistentVolumeClaim:
            claimName: "{{inputs.parameters.pvc-name}}"

      script:
        image: amazon/aws-cli:2.0.6
        imagePullPolicy: "{{workflow.parameters.core-image-pull-strategy}}"
        volumeMounts:
          - name: biondeep-ig-pdb-data
            mountPath: /mnt/data/
        env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: "{{workflow.parameters.storage-secret-name}}"
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: "{{workflow.parameters.storage-secret-name}}"
                key: AWS_SECRET_ACCESS_KEY
          - name: S3_ENDPOINT
            value: "{{workflow.parameters.storage-endpoint}}"
        command: [bash]
        source: |

          yum install -y unzip

          rm -rf /mnt/data/ig-dataset/input
          rm -rf /mnt/data/ig-dataset/artefacts

          if [ -f "/mnt/data/ig-dataset/generated_pmhc.zip" ] ; then
            rm /mnt/data/ig-dataset/generated_pmhc.zip
            echo "Existing archive of pMHCs deleted..."
          fi

          mkdir -p /mnt/data/ig-dataset/input
          mkdir -p /mnt/data/ig-dataset/artefacts

          aws s3 --endpoint-url $S3_ENDPOINT cp {{inputs.parameters.pmhc-structures-s3-path}} /mnt/data/ig-dataset/artefacts/pmhc_structures.zip
          aws s3 --endpoint-url $S3_ENDPOINT cp {{inputs.parameters.pmhc-data-s3-path}} /mnt/data/ig-dataset/input/pmhc_data.csv

          mkdir -p /mnt/data/ig-dataset/artefacts/pmhc

          unzip -j /mnt/data/ig-dataset/artefacts/pmhc_structures.zip -d /mnt/data/ig-dataset/artefacts/pmhc

          rm /mnt/data/ig-dataset/artefacts/pmhc_structures.zip

          ls -lh /mnt/data/ig-dataset/input/

      # <----------------------------------------->

    - name: distribute-data-pmhc-gen
      retryStrategy:
        limit: 5
        retryPolicy: "Always"
      inputs:
        parameters:
          - name: object-bucket
          - name: pvc-name
      volumes:
        - name: biondeep-ig-pdb-data
          persistentVolumeClaim:
            claimName: "{{inputs.parameters.pvc-name}}"

      script:
        image: "{{workflow.parameters.core-image}}"
        imagePullPolicy: "{{workflow.parameters.core-image-pull-strategy}}"
        env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: "{{workflow.parameters.storage-secret-name}}"
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: "{{workflow.parameters.storage-secret-name}}"
                key: AWS_SECRET_ACCESS_KEY
          - name: S3_ENDPOINT
            value: "{{workflow.parameters.storage-endpoint}}"
        volumeMounts:
          - name: biondeep-ig-pdb-data
            mountPath: /mnt/data/
        command: [python]
        source: |
          import pandas as pd
          import sys
          import json

          peptides_list_filepath = "/mnt/data/ig-dataset/input/pmhc_data.csv"
          list = pd.read_csv(peptides_list_filepath)
          list = list.sample(frac=1).reset_index(drop=True)
          todo_alleles=list[['allele','peptide','filename','FLAG']]

          num_alleles = {{workflow.parameters.num-runs-per-parallel-step}}
          num_parallel_job = {{workflow.parameters.num-parallel-steps}}

          rootdir = "/mnt/data/ig-dataset/artefacts"
          partition_list = []

          for i in range(num_parallel_job):
            alleles_part = todo_alleles.iloc[i*num_alleles:(i+1)*num_alleles]

            ale_pep_filename="{}/alleles_peptides_{}_{}.json".format(rootdir, num_alleles, i)
            partition_list.append(ale_pep_filename)

            with open(ale_pep_filename, 'w') as f:
              json.dump(alleles_part.values.tolist(), f)

          json.dump(partition_list, sys.stdout)

        # <----------------------------------------->

    - name: pmhc-gen
      retryStrategy:
        limit: 5
        retryPolicy: "Always"
      inputs:
        parameters:
          - name: object-bucket
          - name: pvc-name
          - name: todo_alleles_peptides_filename
      volumes:
        - name: biondeep-ig-pdb-data
          persistentVolumeClaim:
            claimName: "{{inputs.parameters.pvc-name}}"

      script:
        image: "{{workflow.parameters.core-image}}"
        imagePullPolicy: "{{workflow.parameters.core-image-pull-strategy}}"
        env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: "{{workflow.parameters.storage-secret-name}}"
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: "{{workflow.parameters.storage-secret-name}}"
                key: AWS_SECRET_ACCESS_KEY
          - name: S3_ENDPOINT
            value: "{{workflow.parameters.storage-endpoint}}"
        volumeMounts:
          - name: biondeep-ig-pdb-data
            mountPath: /mnt/data/
        command: [bash]
        source: |

          apt-get update && apt-get install -y tree jq gzip

          # chmod pmhc bash script
          chmod +x ./biondeep_ig/data_gen/pipeline/PosGeneration/gen_pmhc_poses.sh
          # create folder for generated pmhcs
          outdir="/mnt/data/ig-dataset/generated_pmhc"

          # load job tuples from distribute-data-pmhc-gen step
          todo_alleles_peptides={{inputs.parameters.todo_alleles_peptides_filename}}

          uuid=$(cat /proc/sys/kernel/random/uuid)

          if [ ! ${#todo_alleles_peptides[@]} -eq 0 ]; then
            echo "GENERATING COMMANDS..."
            # execute pmhc script for every (allele, peptide) tuple
            for i in $(seq 0 $(expr {{workflow.parameters.num-runs-per-parallel-step}} - 1))
            do

              allele=$(cat $todo_alleles_peptides | jq '.['"$i"'][0] // empty' | tr -d '"')
              peptide=$(cat $todo_alleles_peptides  | jq '.['"$i"'][1] // empty' | tr -d '"')
              init=$(cat $todo_alleles_peptides  | jq '.['"$i"'][2] // empty' | tr -d '"')
              flag=$(cat $todo_alleles_peptides  | jq '.['"$i"'][3] // empty' | tr -d '"')

              if [[ ! -z "$allele" ]]; then
                echo "./biondeep_ig/data_gen/pipeline/PosGeneration/gen_pmhc_poses.sh -r $flag -i $init -a $allele -p $peptide -o $outdir" >> cmds_$uuid
              fi

            done

            cat cmds_$uuid | wc -l
            tail cmds_$uuid

            parallel --progress < cmds_$uuid

            #ls -lh $outdir
            tree $outdir | grep dir
          fi

      podSpecPatch: |
        containers:
          - name: main
            resources:
              requests:
                cpu: "{{workflow.parameters.train-data-generator-cpus}}"
              limits:
                cpu: "{{workflow.parameters.train-data-generator-cpus}}"

    # <----------------------------------------->

    - name: sync-results-to-s3
      retryStrategy:
        limit: 5
        retryPolicy: "Always"
      inputs:
        parameters:
          - name: object-bucket
          - name: pvc-name
      volumes:
        - name: biondeep-ig-pdb-data
          persistentVolumeClaim:
            claimName: "{{inputs.parameters.pvc-name}}"

      script:
        image: "{{workflow.parameters.core-image}}"
        imagePullPolicy: "{{workflow.parameters.core-image-pull-strategy}}"
        env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: "{{workflow.parameters.storage-secret-name}}"
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: "{{workflow.parameters.storage-secret-name}}"
                key: AWS_SECRET_ACCESS_KEY
          - name: S3_ENDPOINT
            value: "{{workflow.parameters.storage-endpoint}}"
        volumeMounts:
          - name: biondeep-ig-pdb-data
            mountPath: /mnt/data/
        command: [bash]
        source: |

          apt-get update
          apt-get -y install tree zip gzip

          pmhc_dir="/mnt/data/ig-dataset/generated_pmhc"
          pmhc_archive="/mnt/data/ig-dataset/generated_pmhc.zip"

          zip -r $pmhc_archive $pmhc_dir

          # datetime
          dt=$(date '+%d_%m_%Y__%H_%M_%S')

          echo "UPLOADING files to S3..."
          echo "{{workflow.parameters.upload-s3-path}}/$dt/generated_pmhc.zip"
          aws s3 --endpoint-url $S3_ENDPOINT cp $pmhc_archive "{{workflow.parameters.upload-s3-path}}/$dt/generated_pmhc.zip"

          # Aggregate scores
          scores_path="/mnt/data/ig-dataset/agg_scores_$dt.csv"
          agg_scores -d $pmhc_dir -o $scores_path
          aws s3 --endpoint-url $S3_ENDPOINT cp $scores_path "{{workflow.parameters.upload-s3-path}}/$dt/pmhc_rosetta_feats_$dt.csv"

          #ls -lh $tcr_pmhc_dir
          ls -lh $pmhc_dir
          #ls -lh $archives_dir/pdb

          #tree $outdir | grep dir
          #tree $archives_dir/pdb | grep dir

      podSpecPatch: |

        containers:
          - name: main
            resources:
              requests:
                cpu: "{{workflow.parameters.train-data-generator-cpus}}"
              limits:
                cpu: "{{workflow.parameters.train-data-generator-cpus}}"
