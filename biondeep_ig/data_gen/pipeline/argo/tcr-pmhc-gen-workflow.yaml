apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: tcr-pmhc-gen-01022022-
  namespace: biondeep-ig-infra
  labels:
    workflows.argoproj.io/controller-instanceid: ichor

spec:
  entrypoint: main

  ########################
  # volume specification #
  ########################

  # volumeClaimTemplates:
  #   - metadata:
  #       name: biondeep-ig-pdb-data
  #     spec:
  #       accessModes: [ "ReadWriteMany"]
  #       storageClassName: rook-ceph-fs
  #       resources:
  #         requests:
  #           storage: 4048Gi

  # ###############################
  # # misc workflow configuration #
  # ###############################

  # Uncomment to delete logs when workflow is completed
  # podGC:
  #   strategy: OnWorkflowCompletion

  ###############################
  # global argument declaration #
  ###############################

  arguments:
    parameters:
      # core docker image configuration
      - name: core-image
        value: "" # Set by the GitLab CI once the Docker image is built
      - name: core-image-pull-strategy
        value: "IfNotPresent"

      # object storage connection
      - name: storage-endpoint
        value: "https://s3.kao.instadeep.io" #"http://192.168.20.1:30930"
      - name: storage-secret-name
        value: "biondeep-data"
      - name: input-data-bucket
        value: "biondeep-data"

      # object storage paths
      - name: upload-s3-path
        value: "s3://biondeep-data/classi_data_for_instadeep/ig_tcr_pmhc"

      - name: tcr-structures-s3-path
        value: "s3://biondeep-data/classi_data_for_instadeep/ig_tcr_pmhc/tcr_structures.zip"

      - name: tcr-pmhc-structures-s3-path
        value: "s3://biondeep-data/classi_data_for_instadeep/ig_tcr_pmhc/tcr_pmhc_structures.zip"

      - name: tcr-pmhc-data-s3-path
        value: "s3://biondeep-data/classi_data_for_instadeep/ig_tcr_pmhc/tcr_pmhc_data.csv"

      # volume config
      - name: pvc-name
        value: "biondeep-ig-data"

      # workflow config
      - name: num-parallel-steps # Number of nodes
        # value: "40"
        value: "23"

      - name: num-runs-per-parallel-step # Number of entries per node
        # value: "13538"
        value: "50868"

      - name: train-data-generator-cpus # Number of cpus per node
        value: "100"

  ###########################
  # workflow implementation #
  ###########################

  templates:
    #######################
    # main DAG definition #
    #######################

    - name: main
      dag:
        tasks:
          - name: pull-data
            template: pull-data
            arguments:
              parameters:
                - name: object-bucket
                  value: "{{workflow.parameters.input-data-bucket}}"
                - name: pvc-name
                  value: "{{workflow.parameters.pvc-name}}"
                - name: tcr-structures-s3-path
                  value: "{{workflow.parameters.tcr-structures-s3-path}}"
                - name: tcr-pmhc-structures-s3-path
                  value: "{{workflow.parameters.tcr-pmhc-structures-s3-path}}"
                - name: tcr-pmhc-data-s3-path
                  value: "{{workflow.parameters.tcr-pmhc-data-s3-path}}"

          - name: distribute-data-tcr-pmhc-gen
            template: distribute-data-tcr-pmhc-gen
            dependencies: [pull-data]
            arguments:
              parameters:
                - name: object-bucket
                  value: "{{workflow.parameters.input-data-bucket}}"
                - name: pvc-name
                  value: "{{workflow.parameters.pvc-name}}"

          - name: tcr-pmhc-gen
            template: tcr-pmhc-gen
            dependencies: [distribute-data-tcr-pmhc-gen]
            arguments:
              parameters:
                - name: object-bucket
                  value: "{{workflow.parameters.input-data-bucket}}"
                - name: pvc-name
                  value: "{{workflow.parameters.pvc-name}}"
                - name: todo_pmhc_tcr_filename
                  value: "{{item}}"
            withParam: "{{tasks.distribute-data-tcr-pmhc-gen.outputs.result}}"

          - name: sync-results-to-s3
            template: sync-results-to-s3
            dependencies: [tcr-pmhc-gen]
            arguments:
              parameters:
                - name: object-bucket
                  value: "{{workflow.parameters.input-data-bucket}}"
                - name: pvc-name
                  value: "{{workflow.parameters.pvc-name}}"

    ##################
    # step templates #
    ##################
    - name: pull-data
      retryStrategy:
        limit: 5
        retryPolicy: "Always"
      inputs:
        parameters:
          - name: object-bucket
          - name: pvc-name
          - name: tcr-structures-s3-path
          - name: tcr-pmhc-structures-s3-path
          - name: tcr-pmhc-data-s3-path
      volumes:
        - name: biondeep-ig-pdb-data
          persistentVolumeClaim:
            claimName: "{{inputs.parameters.pvc-name}}"

      script:
        image: amazon/aws-cli:2.0.6
        imagePullPolicy: "{{workflow.parameters.core-image-pull-strategy}}"
        volumeMounts:
          - name: biondeep-ig-pdb-data
            mountPath: /mnt/data/
        env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: "{{workflow.parameters.storage-secret-name}}"
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: "{{workflow.parameters.storage-secret-name}}"
                key: AWS_SECRET_ACCESS_KEY
          - name: S3_ENDPOINT
            value: "{{workflow.parameters.storage-endpoint}}"
        command: [bash]
        source: |

          yum install -y unzip

          rm -rf /mnt/data/ig-dataset/generated_tcr_pmhc
          mkdir -p /mnt/data/ig-dataset/generated_tcr_pmhc

          if [ -f "/mnt/data/ig-dataset/generated_tcr_pmhc.zip" ] ; then
            rm /mnt/data/ig-dataset/generated_tcr_pmhc.zip
            echo "Existing archive of TCR-pMHCs deleted..."
          fi

          rm -rf /mnt/data/ig-dataset/input
          rm -rf /mnt/data/ig-dataset/artefacts
          mkdir -p /mnt/data/ig-dataset/input
          mkdir -p /mnt/data/ig-dataset/artefacts

          aws s3 --endpoint-url $S3_ENDPOINT  cp {{inputs.parameters.tcr-structures-s3-path}} /mnt/data/ig-dataset/artefacts/tcr_structures.zip

          aws s3 --endpoint-url $S3_ENDPOINT  cp {{inputs.parameters.tcr-pmhc-structures-s3-path}} /mnt/data/ig-dataset/artefacts/tcr_pmhc_structures.zip
          aws s3 --endpoint-url $S3_ENDPOINT  cp {{inputs.parameters.tcr-pmhc-data-s3-path}} /mnt/data/ig-dataset/input/tcr_pmhc_data.csv

          mkdir -p /mnt/data/ig-dataset/artefacts/tcr
          mkdir -p /mnt/data/ig-dataset/artefacts/template

          unzip -j /mnt/data/ig-dataset/artefacts/tcr_structures.zip -d /mnt/data/ig-dataset/artefacts/tcr
          unzip -j /mnt/data/ig-dataset/artefacts/tcr_pmhc_structures.zip -d /mnt/data/ig-dataset/artefacts/template

          rm /mnt/data/ig-dataset/artefacts/tcr_structures.zip
          rm /mnt/data/ig-dataset/artefacts/tcr_pmhc_structures.zip

          ls -lh /mnt/data/ig-dataset/input/

      # <----------------------------------------->

    - name: distribute-data-tcr-pmhc-gen
      retryStrategy:
        limit: 5
        retryPolicy: "Always"
      inputs:
        parameters:
          - name: object-bucket
          - name: pvc-name
      volumes:
        - name: biondeep-ig-pdb-data
          persistentVolumeClaim:
            claimName: "{{inputs.parameters.pvc-name}}"

      script:
        image: "{{workflow.parameters.core-image}}"
        imagePullPolicy: "{{workflow.parameters.core-image-pull-strategy}}"
        env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: "{{workflow.parameters.storage-secret-name}}"
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: "{{workflow.parameters.storage-secret-name}}"
                key: AWS_SECRET_ACCESS_KEY
          - name: S3_ENDPOINT
            value: "{{workflow.parameters.storage-endpoint}}"
        volumeMounts:
          - name: biondeep-ig-pdb-data
            mountPath: /mnt/data/
        command: [python]
        source: |
          import pandas as pd
          import sys
          import json

          # load csv with pmhc filename, tcrs and templates
          data_filepath = "/mnt/data/ig-dataset/input/tcr_pmhc_data.csv"
          data = pd.read_csv(data_filepath)
          data = data.sample(frac=1).reset_index(drop=True)
          df=data[['pmhc_filename','tcr_filename','template_filename']]

          num_run = {{workflow.parameters.num-runs-per-parallel-step}}
          num_parallel_job = {{workflow.parameters.num-parallel-steps}}

          rootdir = "/mnt/data/ig-dataset/artefacts"
          partition_list = []

          for i in range(num_parallel_job):
            part = df.iloc[i*num_run:(i+1)*num_run]

            ale_pep_filename="{}/alleles_peptides_{}_{}.json".format(rootdir, num_run, i)
            partition_list.append(ale_pep_filename)

            with open(ale_pep_filename, 'w') as f:
              json.dump(part.values.tolist(), f)

          json.dump(partition_list, sys.stdout)

        # <----------------------------------------->

    - name: tcr-pmhc-gen
      retryStrategy:
        limit: 5
        retryPolicy: "Always"
      inputs:
        parameters:
          - name: object-bucket
          - name: pvc-name
          - name: todo_pmhc_tcr_filename
      volumes:
        - name: biondeep-ig-pdb-data
          persistentVolumeClaim:
            claimName: "{{inputs.parameters.pvc-name}}"

      script:
        image: "{{workflow.parameters.core-image}}"
        imagePullPolicy: "{{workflow.parameters.core-image-pull-strategy}}"
        env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: "{{workflow.parameters.storage-secret-name}}"
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: "{{workflow.parameters.storage-secret-name}}"
                key: AWS_SECRET_ACCESS_KEY
          - name: S3_ENDPOINT
            value: "{{workflow.parameters.storage-endpoint}}"
        volumeMounts:
          - name: biondeep-ig-pdb-data
            mountPath: /mnt/data/
        command: [bash]
        source: |

          apt-get update && apt-get install -y tree jq gzip

          chmod +x ./biondeep_ig/data_gen/pipeline/PosGeneration/gen_tcr_pmhc_poses.sh

          # create folder for generated tcr pmhcs
          outdir="/mnt/data/ig-dataset/generated_tcr_pmhc"

          todos="{{inputs.parameters.todo_pmhc_tcr_filename}}"

          uuid=$(cat /proc/sys/kernel/random/uuid)

          if [ ! ${#todos[@]} -eq 0 ]; then
            for i in $(seq 0 $(expr {{workflow.parameters.num-runs-per-parallel-step}} - 1))
            do

              pmhc=$(cat $todos  | jq '.['"$i"'][0] // empty' | tr -d '"')
              tcr=$(cat $todos  | jq '.['"$i"'][1] // empty' | tr -d '"')
              template=$(cat $todos  | jq '.['"$i"'][2] // empty' | tr -d '"')

              if [[ ! -z "$pmhc" ]]; then
                echo "./biondeep_ig/data_gen/pipeline/PosGeneration/gen_tcr_pmhc_poses.sh -p $pmhc -r $tcr -t $template -o $outdir -n 4"  >> cmds_$uuid
              fi

            done

            cat cmds_$uuid | wc -l
            tail cmds_$uuid

            parallel --progress < cmds_$uuid

            #ls -lh $outdir
            tree $outdir
          fi

      podSpecPatch: |
        containers:
          - name: main
            resources:
              requests:
                cpu: "{{workflow.parameters.train-data-generator-cpus}}"
              limits:
                cpu: "{{workflow.parameters.train-data-generator-cpus}}"

    # <----------------------------------------->

    - name: sync-results-to-s3
      retryStrategy:
        limit: 5
        retryPolicy: "Always"
      inputs:
        parameters:
          - name: object-bucket
          - name: pvc-name
      volumes:
        - name: biondeep-ig-pdb-data
          persistentVolumeClaim:
            claimName: "{{inputs.parameters.pvc-name}}"

      script:
        image: "{{workflow.parameters.core-image}}"
        imagePullPolicy: "{{workflow.parameters.core-image-pull-strategy}}"
        env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: "{{workflow.parameters.storage-secret-name}}"
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: "{{workflow.parameters.storage-secret-name}}"
                key: AWS_SECRET_ACCESS_KEY
          - name: S3_ENDPOINT
            value: "{{workflow.parameters.storage-endpoint}}"
        volumeMounts:
          - name: biondeep-ig-pdb-data
            mountPath: /mnt/data/
        command: [bash]
        source: |

          apt-get update
          apt-get -y install tree zip gzip

          tcr_pmhc_dir="/mnt/data/ig-dataset/generated_tcr_pmhc"
          tcr_pmhc_archive="/mnt/data/ig-dataset/generated_tcr_pmhc.zip"

          zip -r $tcr_pmhc_archive $tcr_pmhc_dir

          # datetime
          dt=$(date '+%d_%m_%Y__%H_%M_%S')

          echo "DESTINATION S3"
          echo "{{workflow.parameters.upload-s3-path}}/$dt/generated_tcr_pmhc.zip"
          aws s3 --endpoint-url $S3_ENDPOINT cp $tcr_pmhc_archive "{{workflow.parameters.upload-s3-path}}/$dt/generated_tcr_pmhc.zip"

          # Aggregate scores
          scores_path="/mnt/data/ig-dataset/tcr_pmhc_rosetta_feats_$dt.csv"
          agg_scores -d $tcr_pmhc_dir -o $scores_path
          aws s3 --endpoint-url $S3_ENDPOINT cp $scores_path "{{workflow.parameters.upload-s3-path}}/$dt/tcr_pmhc_rosetta_feats_$dt.csv"

          df -h
          #ls -lh $tcr_pmhc_dir
          #ls -lh $pmhc_dir
          #ls -lh $archives_dir/pdb

          tree $outdir | grep dir
          #tree $archives_dir/pdb | grep dir

      podSpecPatch: |
        containers:
          - name: main
            resources:
              requests:
                cpu: "{{workflow.parameters.train-data-generator-cpus}}"
              limits:
                cpu: "{{workflow.parameters.train-data-generator-cpus}}"
