model_type: LLMModel # LLMModel, LLMMixedModel
model_config:
  general_params:
    # LLM model path from HuggingFace (Protein models: ESM / protBERT or DNA models: NT models or RNA models: RiNALMoModel)
    llm_hf_model_path: # multimolecule/rinalmo # InstaDeepAI/nucleotide-transformer-v2-500m-multi-species # Rostlab/prot_bert_bfd  # facebook/esm2_t33_650M_UR50D
    model_source: # HF library under which the model is implemented: AutoModel, RiNALMoModel, AutoModelForMaskedLM
    tokenizer_source: # HF library under which the tokenizer is implemented: AutoTokenizer, RnaTokenizer
    training_type: finetuning
    # wild type and mutated/tested sequences (column name used to load sequences from csv files)
    wildtype_col_name:
    mutated_col_name:
    mutation_position_col_name: #(Optional) Used with context_length to crop long DNA/RNA sequences around mutation position
    save_llm_only: True # Only used with LLMModel to later use the finetuned LLM backbone in LLMMixedModel
    pretrained_llm_path: # None, Path to finetuned LLM backbone (Only used with LLMMixedModel)
    # ML model params (only needed for LLMMixedModel)
    num_boost_round: 20000000
    verbose_eval: 10
    early_stopping_rounds: 15
    lr_scheduler_config:
      use_scheduler: True
      max_lr: 0.003
  model_params:
    context_length: # supported sequence length on base pairs
    aggregation: mean # mean: average of all tokens , mut_token: embedding of the mutated token
    embed_dim: # 1024 for protBERT & NT 500m, 512 for NT 50m, 2560 for ESM2 3B and 1280 for ESM2 650M and 320 for esm2_t6_8M_UR50D
    mlp_layer_sizes: #[2048, 1]
    k_for_kmers: 6
    batch_size: 16
    batch_size_test: 16
    shuffle_dataloader: True
    num_workers: 5
    lr: 0.0003
    val_frequency: 0.05 # Should be a percentage (0 to 1)
    early_stop_patience: 700 #
    early_stop_metric: top_k # available validation metrics : accuracy, top_k, f1, roc, loss
    early_stop_metric_max: True # True if early_stop_metric should be maximized ( False for loss)
    num_epochs: 20
    threshold: 0.5
    seed: 2020
    ml_model_params: # XGboost params used in case of LLMMixedModel
      alpha: 0.02
      booster: gbtree
      colsample_bytree: 1
      eta: 0.01
      eval_metric: logloss
      gamma: 0.4
      lambda: 5
      max_depth: 7
      min_child_weight: 19
      nthread: 32
      objective: binary:logistic
      seed: 2020
      subsample: 1
