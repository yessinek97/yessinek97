model_type: XgboostModel
model_config:
  general_params:
    num_boost_round: 10
    verbose_eval: 10
    early_stopping_rounds: 15
  model_params:
    alpha: 0.02
    booster: gbtree
    colsample_bytree: 1
    eta: 0.01
    eval_metric: logloss
    gamma: 0.4
    lambda: 5
    max_depth: 7
    min_child_weight: 19
    nthread: 32
    objective: binary:logistic
    seed: 2020
    subsample: 1
