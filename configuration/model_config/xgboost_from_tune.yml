model_type: XgboostModel
model_config:
  general_params:
    early_stopping_rounds: 15
    num_boost_round: 10000000
    verbose_eval: false
  model_params:
    alpha: 0.009000000000000001
    booster: gbtree
    colsample_bytree: 1
    eta: 0.03
    eval_metric: logloss
    gamma: 0.0
    lambda: 0.1
    max_delta_step: 1
    max_depth: 7
    min_child_weight: 1
    nthread: 16
    objective: binary:logistic
    seed: 1994
    subsample: 1
    tree_method: auto
