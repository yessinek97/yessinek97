model_type: LLMModel # LLMModel, LLMMixedModel
model_config:
  general_params:
    # LLM model path from HuggingFace (Protein models: ESM / protBERT or DNA models: NT models)
    llm_hf_model_path: "multimolecule/rinalmo" # "InstaDeepAI/nucleotide-transformer-v2-500m-multi-species" # Rostlab/prot_bert_bfd  # facebook/esm2_t33_650M_UR50D
    model_source: RiNALMoModel # HF library under which the model is implemented: AutoModel, RiNALMoModel
    tokenizer_source: RnaTokenizer # HF library under which the tokenizer is impemented: AutoTokenizer, RnaTokenizer
    training_type: peft #peft, finetuning, probing
    # wild type and mutated/tested sequences (column name used to load sequences from csv file and embeddings file)
    wildtype_col_name: wt_rna_sequence_27mer
    mutated_col_name: mut_rna_sequence_27mer
    mutation_position_col_name: mutation_start_position_27mer
    save_llm_only: True # Only used with LLMModel
    pretrained_llm_path: # None, Path (Only used with LLMMixedModel)
    # ML model params (only needed for LLMMixedModel)
    num_boost_round: 20000000
    verbose_eval: 10
    early_stopping_rounds: 15
    lr_scheduler_config:
      use_scheduler: False
      max_lr: 0.003
  model_params:
    context_length: # supported sequence length on base pairs
    aggregation: mean # mean: average of all tokens , mut_token: embedding of the mutated token
    embed_dim: 1280 # 1024 for protBERT & NT 500m, 512 for NT 50m, 2560 for ESM2 3B and 1280 for ESM2 650M and 320 for esm2_t6_8M_UR50D
    mlp_layer_sizes: #[2048, 1]
    k_for_kmers: 6
    batch_size: 16
    batch_size_test: 16
    shuffle_dataloader: True
    num_workers: 3
    lr: 0.0003
    val_frequency: 0.05 # Should be a percentage (0 to 1)
    early_stop_patience: 700 #
    early_stop_metric: top_k # available validation metrics : accuracy, top_k, f1, roc, loss
    early_stop_metric_max: True # True if early_stop_metric should be maximized ( False for loss)
    num_epochs: 20
    threshold: 0.5
    seed: 2020
    lora_config: # used in case of doing PEFT
      r: 1
      lora_alpha: 2
      lora_dropout: 0.1
      target_modules: [query, key, value]
    ml_model_params: # used in case of using LLMMixedModel
      alpha: 0.02
      booster: gbtree
      colsample_bytree: 1
      eta: 0.01
      eval_metric: logloss
      gamma: 0.4
      lambda: 5
      max_depth: 7
      min_child_weight: 19
      nthread: 32
      objective: binary:logistic
      seed: 2020
      subsample: 1
