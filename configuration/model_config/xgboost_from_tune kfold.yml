model_type: XgboostModel
model_config:
  general_params:
    early_stopping_rounds: 15
    num_boost_round: 10000000
    save_model: True
    verbose_eval: false
  model_params:
    alpha: 0.47000000000000003
    booster: gbtree
    colsample_bytree: 1
    eta: 0.029
    eval_metric: logloss
    gamma: 0.0
    lambda: 1.96
    max_delta_step: 9
    max_depth: 7
    min_child_weight: 0
    nthread: -1
    objective: binary:logistic
    seed: 2020
    subsample: 1
    tree_method: auto
